Abstract
â€œTo Build an end-to-end data pipeline where raw data is ingested from Azure Data Lake using Azure Data Factory into Snowflake, transformed using dbt following medallion architecture, and exposed for analytics.â€
Source Files (CSV/JSON)
        â†“
Azure Data Lake Gen2
        â†“
Azure Data Factory (Copy Activity)
        â†“
Snowflake RAW schema
        â†“
dbt (staging â†’ intermediate â†’ marts)
        â†“
Analytics-ready tables
Methodology
Create 3 sample CSV files:
customers.csv
customer_id,customer_name,region
1,Ali,South
2,John,North
products.csv
product_id,product_name,category
101,Laptop,Electronics
102,Phone,Electronics
orders.csv
order_id,customer_id,product_id,quantity,price,order_date
1001,1,101,1,50000,2024-01-10
1002,2,102,2,30000,2024-01-12

Step 2: Snowflake Setup
Database & Schemas
CREATE DATABASE SALES_DB;

CREATE SCHEMA RAW;
CREATE SCHEMA STAGING;
CREATE SCHEMA MART;
RAW Tables (Loaded by ADF)
CREATE TABLE RAW.customers (...);
CREATE TABLE RAW.products (...);
CREATE TABLE RAW.orders (...);
ğŸ‘‰ At this stage:
â€¢	No joins
â€¢	No calculations
â€¢	Just raw data


Step 3: Azure Data Factory (ADF)
â€œADF is used only for ingestion; transformations are pushed to Snowflake using dbt.â€
Create the Storage Account first, then create Azure Data Factory.
Why this order makes sense
â€¢	Storage Account = where the data lives (Blob, ADLS, files, etc.)
â€¢	Data Factory = the tool that moves / transforms that data
ADF doesnâ€™t store your data itself â€” it only connects to existing sources and destinations.
So when you create Linked Services inside ADF, it needs something already there to connect to.

Step 1: Open Azure Data Factory
1.	Login to Azure Portal
2.	Search Data Factories
3.	Open your Data Factory
4.	Click Launch Studio
________________________________________
Step 2: Create Linked Services (Connections)
You need connections for source and destination.
Example: Blob â†’ SQL
1.	Go to Manage (ğŸ”§) â†’ Linked services
2.	Click New
3.	Choose Azure Blob Storage
o	Select subscription
o	Choose storage account
o	Test connection â†’ Create
4.	Again click New
5.	Choose Azure SQL Database(snowflake)
o	Enter server, DB, username, password
o	Test connection â†’ Create
Step 3: Create Datasets (Data structure)
Source Dataset (Blob)
1.	Go to Author (âœï¸) â†’ + â†’ Dataset
2.	Select Azure Blob Storage
3.	Choose file format (CSV / JSON / Parquet)
4.	Select container and file
5.	Import schema â†’ Save
Sink Dataset (SQL)
1.	+ â†’ Dataset
2.	Choose Azure SQL Database
3.	Select table
4.	Import schema â†’ Save
Step 4: Create Pipeline
1.	Go to Author (âœï¸)
2.	Click + â†’ Pipeline
3.	Drag Copy Data activity into canvas
________________________________________
Step 5: Configure Copy Activity
1.	Click Copy Data
2.	Go to Source tab
o	Select Blob dataset
3.	Go to Sink tab
o	Select SQL dataset
4.	Mapping tab
o	Auto-map or manual map columns
________________________________________
Step 6: Validate & Debug
1.	Click Validate
2.	Click Debug
3.	Wait for Succeeded status
Databuild tool
Step 4: dbt Project Setup
Create dbt project connected to Snowflake.
Folder Structure
models/
â”‚
â”œâ”€â”€ staging/
â”‚   â”œâ”€â”€ stg_customers.sql
â”‚   â”œâ”€â”€ stg_products.sql
â”‚   â””â”€â”€ stg_orders.sql
â”‚
â”œâ”€â”€ marts/
â”‚   â””â”€â”€ sales_fact.sql
________________________________________
ğŸ”¹ Step 5: dbt Staging Models
stg_customers.sql
SELECT
  customer_id,
  customer_name,
  region
FROM {{ source('raw', 'customers') }}
stg_orders.sql
SELECT
  order_id,
  customer_id,
  product_id,
  quantity,
  price,
  quantity * price AS total_amount,
  order_date
FROM {{ source('raw', 'orders') }}

stg_products.sql

SELECT
  product_id,
  product_name,
  category
FROM {{ source('raw', 'products') }}
ğŸ“Œ Purpose:
â€¢	Clean
â€¢	Rename columns
â€¢	Add derived fields
________________________________________
ğŸ§± Step 6: dbt Mart Model (Business Logic)
sales_fact.sql
SELECT
  o.order_id,
  c.customer_name,
  c.region,
  p.product_name,
  p.category,
  o.quantity,
  o.total_amount,
  o.order_date
FROM {{ ref('stg_orders') }} o
JOIN {{ ref('stg_customers') }} c
  ON o.customer_id = c.customer_id
JOIN {{ ref('stg_products') }} p
  ON o.product_id = p.product_id
ğŸ“Œ Interview keywords:
â€¢	Fact table
â€¢	Business-ready
â€¢	Analytics optimized
________________________________________
âœ… Step 7: dbt Tests (Very Important)
version: 2

models:
  - name: stg_orders
    columns:
      - name: order_id
        tests:
          - not_null
          - unique
Run:
dbt test
dbt run
________________________________________
ğŸ“Š Step 8: Final Output
Business users can query:
SELECT region, SUM(total_amount)
FROM MART.sales_fact
GROUP BY region;
________________________________________
Learning outcomes
Skill	Covered
ADF ingestion	âœ…
Snowflake schemas	âœ…
ELT architecture	âœ…
dbt refs & sources	âœ…
Testing	
Interview storytelling	âœ…
	âœ…
________________________________________



